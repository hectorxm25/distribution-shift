{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment A2: Activation Patching with Counterfactuals\n",
    "\n",
    "Same setup as A1\n",
    "TODO: record adversarial label to test label correctness\n",
    "\n",
    "https://nnsight.net/notebooks/tutorials/activation_patching/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnsight\n",
    "from nnsight import NNsight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from robustness import model_utils\n",
    "from robustness.datasets import CIFAR\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# Add the parent directory to the path to import custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from models.resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cpu\n"
     ]
    }
   ],
   "source": [
    "# load saved model\n",
    "datapath = '/u/yshi23/distribution-shift/datasets'\n",
    "model_pth = \"/u/yshi23/distribution-shift/adversarial/out/checkpoint.pt.best\"\n",
    "device = \"cpu\"\n",
    "\n",
    "# model init using robustness mechanism\n",
    "# model, _ = model_utils.make_and_restore_model(\n",
    "#     arch='resnet18', dataset=CIFAR(datapath), resume_path=model_pth, device=device\n",
    "# )\n",
    "# Load the checkpoint with map_location to ensure it's on the right device\n",
    "model = ResNet18(num_classes=10)\n",
    "checkpoint = torch.load(model_pth, map_location=device)\n",
    "\n",
    "# Get the state dict\n",
    "state_dict = checkpoint[\"model\"]\n",
    "\n",
    "# Create a new state dict with properly formatted keys\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    # Remove 'module.' prefix if present\n",
    "    if k.startswith('module.'):\n",
    "        name = k[7:]  # remove 'module.'\n",
    "    else:\n",
    "        name = k\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "# Load the modified state dict with strict=False to ignore missing/unexpected keys\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "print(\"Model loaded successfully on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_image(image_path):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Define the transforms that match ResNet18 expectations\n",
    "    # ResNet18 expects 224x224 images, but CIFAR-10 is 32x32\n",
    "    # We'll resize and normalize according to ImageNet stats as typically used with ResNet\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(32),  # Resize to 224x224\n",
    "        transforms.ToTensor(),   # Convert to tensor\n",
    "        transforms.Normalize(          # Normalize with CIFAR-10 stats\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Apply transformations\n",
    "    img_tensor = transform(img)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_tensor = img_tensor.unsqueeze(0)  # Shape: [1, 3, 224, 224]\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "# Usage example\n",
    "orig_path = \"images/natural_image_0.png\"\n",
    "clean_img = load_cifar10_image(orig_path)\n",
    "\n",
    "low_ep_path = \"images/mask_small_epsilon_0.png\"\n",
    "low_eps_img = load_cifar10_image(low_ep_path)\n",
    "\n",
    "high_ep_path = \"images/mask_large_epsilon_0.png\"\n",
    "high_eps_img = load_cifar10_image(high_ep_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model layer names:\n",
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.num_batches_tracked\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.num_batches_tracked\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.bn1.running_mean\n",
      "layer1.1.bn1.running_var\n",
      "layer1.1.bn1.num_batches_tracked\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer1.1.bn2.running_mean\n",
      "layer1.1.bn2.running_var\n",
      "layer1.1.bn2.num_batches_tracked\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.num_batches_tracked\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.num_batches_tracked\n",
      "layer2.0.shortcut.0.weight\n",
      "layer2.0.shortcut.1.weight\n",
      "layer2.0.shortcut.1.bias\n",
      "layer2.0.shortcut.1.running_mean\n",
      "layer2.0.shortcut.1.running_var\n",
      "layer2.0.shortcut.1.num_batches_tracked\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.bn1.running_mean\n",
      "layer2.1.bn1.running_var\n",
      "layer2.1.bn1.num_batches_tracked\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer2.1.bn2.running_mean\n",
      "layer2.1.bn2.running_var\n",
      "layer2.1.bn2.num_batches_tracked\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.num_batches_tracked\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.num_batches_tracked\n",
      "layer3.0.shortcut.0.weight\n",
      "layer3.0.shortcut.1.weight\n",
      "layer3.0.shortcut.1.bias\n",
      "layer3.0.shortcut.1.running_mean\n",
      "layer3.0.shortcut.1.running_var\n",
      "layer3.0.shortcut.1.num_batches_tracked\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.bn1.running_mean\n",
      "layer3.1.bn1.running_var\n",
      "layer3.1.bn1.num_batches_tracked\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer3.1.bn2.running_mean\n",
      "layer3.1.bn2.running_var\n",
      "layer3.1.bn2.num_batches_tracked\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.num_batches_tracked\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.num_batches_tracked\n",
      "layer4.0.shortcut.0.weight\n",
      "layer4.0.shortcut.1.weight\n",
      "layer4.0.shortcut.1.bias\n",
      "layer4.0.shortcut.1.running_mean\n",
      "layer4.0.shortcut.1.running_var\n",
      "layer4.0.shortcut.1.num_batches_tracked\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.bn1.running_mean\n",
      "layer4.1.bn1.running_var\n",
      "layer4.1.bn1.num_batches_tracked\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "layer4.1.bn2.running_mean\n",
      "layer4.1.bn2.running_var\n",
      "layer4.1.bn2.num_batches_tracked\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    }
   ],
   "source": [
    "# Get all layer names (keys) from the model's state dictionary\n",
    "layer_names = list(model.state_dict().keys())\n",
    "\n",
    "# Print all layer names\n",
    "print(\"Model layer names:\")\n",
    "for name in layer_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edited:  tensor([[ 0.4219,  0.2396,  0.4102,  0.8014,  0.3155,  0.1142,  0.5998, -0.8716,\n",
      "          0.9336,  0.4383]], grad_fn=<AddmmBackward0>)\n",
      "original:  tensor([[ 0.4219,  0.2396,  0.4102,  0.8014,  0.3155,  0.1142,  0.5998, -0.8716,\n",
      "          0.9336,  0.4383]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with nnsight_model.trace(low_eps_img) as tracer:\n",
    "    # Forward pass through the model\n",
    "    # low eps adversarial\n",
    "\n",
    "    conv1_out = nnsight_model.conv1.output.save()\n",
    "    layer1_out = nnsight_model.layer1.output.save()\n",
    "    layer2_out = nnsight_model.layer2.output.save()\n",
    "    layer3_out = nnsight_model.layer3.output.save()\n",
    "    layer4_out = nnsight_model.layer4.output.save()\n",
    "    linear_out = nnsight_model.linear.output.save()\n",
    "\n",
    "# edit the og model first with conv1_out\n",
    "with nnsight_model.edit() as low_eps_conv1_edit:\n",
    "    nnsight_model.layer4.output = layer4_out\n",
    "\n",
    "# now get the prediction confidence of the edited model \n",
    "print(\"edited: \", low_eps_conv1_edit(clean_img))\n",
    "print(\"original: \", nnsight_model(clean_img))\n",
    "print(\"original with adv input: \", nnsight_model(low_eps_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available top-level attributes: ['R', 'add', 'apply', 'args', 'backend', 'cond', 'execute', 'graph', 'invoke', 'invoker', 'is_protocol', 'iter', 'kwargs', 'local', 'log', 'stop', 'style', 'vis']\n",
      "Error accessing sublayers: 'InterleavingTracer' object has no attribute 'layer1'\n"
     ]
    }
   ],
   "source": [
    "orig_class_idx = 0\n",
    "small_eps_class_idx = 4\n",
    "large_eps_class_idx = 6\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
