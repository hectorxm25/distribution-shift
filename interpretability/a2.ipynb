{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment A2: Activation Patching with Counterfactuals\n",
    "\n",
    "Same setup as A1\n",
    "TODO: record adversarial label to test label correctness\n",
    "\n",
    "https://nnsight.net/notebooks/tutorials/activation_patching/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 19:16:16.097800: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import nnsight\n",
    "from nnsight import NNsight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from robustness import model_utils\n",
    "from robustness.datasets import CIFAR\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# Add the parent directory to the path to import custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from models.resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cpu\n"
     ]
    }
   ],
   "source": [
    "# load saved model\n",
    "datapath = '/u/yshi23/distribution-shift/datasets'\n",
    "model_pth = \"/u/yshi23/distribution-shift/adversarial/out/checkpoint.pt.best\"\n",
    "device = \"cpu\"\n",
    "\n",
    "# model init using robustness mechanism\n",
    "# model, _ = model_utils.make_and_restore_model(\n",
    "#     arch='resnet18', dataset=CIFAR(datapath), resume_path=model_pth, device=device\n",
    "# )\n",
    "# Load the checkpoint with map_location to ensure it's on the right device\n",
    "model = ResNet18(num_classes=10)\n",
    "checkpoint = torch.load(model_pth, map_location=device)\n",
    "\n",
    "# Get the state dict\n",
    "state_dict = checkpoint[\"model\"]\n",
    "\n",
    "# Create a new state dict with properly formatted keys\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    # Remove 'module.' prefix if present\n",
    "    if k.startswith('module.'):\n",
    "        name = k[7:]  # remove 'module.'\n",
    "    else:\n",
    "        name = k\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "# Load the modified state dict with strict=False to ignore missing/unexpected keys\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "print(\"Model loaded successfully on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_image(image_path):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Define the transforms that match ResNet18 expectations\n",
    "    # ResNet18 expects 224x224 images, but CIFAR-10 is 32x32\n",
    "    # We'll resize and normalize according to ImageNet stats as typically used with ResNet\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(32),  # Resize to 224x224\n",
    "        transforms.ToTensor(),   # Convert to tensor\n",
    "        transforms.Normalize(          # Normalize with CIFAR-10 stats\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Apply transformations\n",
    "    img_tensor = transform(img)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_tensor = img_tensor.unsqueeze(0)  # Shape: [1, 3, 224, 224]\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "# Usage example\n",
    "orig_path = \"images/natural_image_0.png\"\n",
    "clean_img = load_cifar10_image(orig_path)\n",
    "\n",
    "low_ep_path = \"images/mask_small_epsilon_0.png\"\n",
    "low_eps_img = load_cifar10_image(low_ep_path)\n",
    "\n",
    "high_ep_path = \"images/mask_large_epsilon_0.png\"\n",
    "high_eps_img = load_cifar10_image(high_ep_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model layer names:\n",
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.num_batches_tracked\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.num_batches_tracked\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.bn1.running_mean\n",
      "layer1.1.bn1.running_var\n",
      "layer1.1.bn1.num_batches_tracked\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer1.1.bn2.running_mean\n",
      "layer1.1.bn2.running_var\n",
      "layer1.1.bn2.num_batches_tracked\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.num_batches_tracked\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.num_batches_tracked\n",
      "layer2.0.shortcut.0.weight\n",
      "layer2.0.shortcut.1.weight\n",
      "layer2.0.shortcut.1.bias\n",
      "layer2.0.shortcut.1.running_mean\n",
      "layer2.0.shortcut.1.running_var\n",
      "layer2.0.shortcut.1.num_batches_tracked\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.bn1.running_mean\n",
      "layer2.1.bn1.running_var\n",
      "layer2.1.bn1.num_batches_tracked\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer2.1.bn2.running_mean\n",
      "layer2.1.bn2.running_var\n",
      "layer2.1.bn2.num_batches_tracked\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.num_batches_tracked\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.num_batches_tracked\n",
      "layer3.0.shortcut.0.weight\n",
      "layer3.0.shortcut.1.weight\n",
      "layer3.0.shortcut.1.bias\n",
      "layer3.0.shortcut.1.running_mean\n",
      "layer3.0.shortcut.1.running_var\n",
      "layer3.0.shortcut.1.num_batches_tracked\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.bn1.running_mean\n",
      "layer3.1.bn1.running_var\n",
      "layer3.1.bn1.num_batches_tracked\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer3.1.bn2.running_mean\n",
      "layer3.1.bn2.running_var\n",
      "layer3.1.bn2.num_batches_tracked\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.num_batches_tracked\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.num_batches_tracked\n",
      "layer4.0.shortcut.0.weight\n",
      "layer4.0.shortcut.1.weight\n",
      "layer4.0.shortcut.1.bias\n",
      "layer4.0.shortcut.1.running_mean\n",
      "layer4.0.shortcut.1.running_var\n",
      "layer4.0.shortcut.1.num_batches_tracked\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.bn1.running_mean\n",
      "layer4.1.bn1.running_var\n",
      "layer4.1.bn1.num_batches_tracked\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "layer4.1.bn2.running_mean\n",
      "layer4.1.bn2.running_var\n",
      "layer4.1.bn2.num_batches_tracked\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    }
   ],
   "source": [
    "# Get all layer names (keys) from the model's state dictionary\n",
    "layer_names = list(model.state_dict().keys())\n",
    "\n",
    "# Print all layer names\n",
    "print(\"Model layer names:\")\n",
    "for name in layer_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnsight_model = NNsight(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extremely edited:  tensor([[ 0.6504,  0.8534, -1.1871,  0.7822, -0.0573,  0.2202, -0.6878, -0.3623,\n",
      "          0.5581, -0.2089]], grad_fn=<AddmmBackward0>)\n",
      "layer1 edited:  tensor([[ 0.6504,  0.8534, -1.1871,  0.7822, -0.0573,  0.2202, -0.6878, -0.3623,\n",
      "          0.5581, -0.2089]], grad_fn=<AddmmBackward0>)\n",
      "layer2 edited:  tensor([[ 0.6504,  0.8534, -1.1871,  0.7822, -0.0573,  0.2202, -0.6878, -0.3623,\n",
      "          0.5581, -0.2089]], grad_fn=<AddmmBackward0>)\n",
      "linear edited:  tensor([[ 0.6504,  0.8534, -1.1871,  0.7822, -0.0573,  0.2202, -0.6878, -0.3623,\n",
      "          0.5581, -0.2089]], grad_fn=<AddmmBackward0>)\n",
      "original:  tensor([[ 0.6504,  0.8534, -1.1871,  0.7822, -0.0573,  0.2202, -0.6878, -0.3623,\n",
      "          0.5581, -0.2089]], grad_fn=<AddmmBackward0>)\n",
      "original with adv input:  tensor([[ 0.6713,  0.7972, -1.0669,  0.8259,  0.0118,  0.2595, -0.6503, -0.4557,\n",
      "          0.5525, -0.2232]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# First trace\n",
    "with nnsight_model.trace(high_eps_img) as tracer:\n",
    "    conv1_out = nnsight_model.conv1.output.save()\n",
    "    layer1_out = nnsight_model.layer1.output.save()\n",
    "    layer2_out = nnsight_model.layer2.output.save()\n",
    "    layer3_out = nnsight_model.layer3.output.save()\n",
    "    layer4_out = nnsight_model.layer4.output.save()\n",
    "    linear_out = nnsight_model.linear.output.save()\n",
    "\n",
    "# Edit layer1\n",
    "with nnsight_model.edit() as layer1_edit:\n",
    "    nnsight_model.layer1.output = layer1_out\n",
    "\n",
    "# Edit layer2\n",
    "with nnsight_model.edit() as layer2_edit:\n",
    "    nnsight_model.layer2.output = layer2_out\n",
    "\n",
    "with nnsight_model.edit() as linear_edit:\n",
    "    nnsight_model.linear.output = linear_out\n",
    "\n",
    "# Test if patching works at all\n",
    "with nnsight_model.edit() as extreme_edit:\n",
    "    # Replace with zeros or significantly altered values\n",
    "    nnsight_model.layer1.output = layer1_out * 100  # Multiply by 100 to create extreme change\n",
    "\n",
    "print(\"extremely edited: \", extreme_edit(clean_img))\n",
    "\n",
    "# Compare results\n",
    "print(\"layer1 edited: \", layer1_edit(clean_img))\n",
    "print(\"layer2 edited: \", layer2_edit(clean_img))\n",
    "print(\"linear edited: \", linear_edit(clean_img))\n",
    "print(\"original: \", nnsight_model(clean_img))\n",
    "print(\"original with adv input: \", nnsight_model(high_eps_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _save_main_layers(model):\n",
    "    return {\n",
    "        \"conv1\": model.conv1.output.save(),\n",
    "        \"bn1\": model.bn1.output.save(),\n",
    "        \"layer1\": model.layer1.output.save(),\n",
    "        \"layer2\": model.layer2.output.save(),\n",
    "        \"layer3\": model.layer3.output.save(),\n",
    "        \"layer4\": model.layer4.output.save(),\n",
    "        \"linear\": model.linear.output.save(),\n",
    "    }\n",
    "\n",
    "\n",
    "def _save_block_outputs(layer, prefix):\n",
    "    return {f\"{prefix}.block{i}\": block.output.save() for i, block in enumerate(layer)}\n",
    "\n",
    "\n",
    "def _compute_diffs(clean_act, adv_act):\n",
    "    return {\n",
    "        \"mean\": torch.mean(torch.abs(adv_act - clean_act)).item(),\n",
    "        \"max\": torch.max(torch.abs(adv_act - clean_act)).item(),\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_activation_differences(model, clean_img, adv_img):\n",
    "    \"\"\"\n",
    "    Visualize differences in activations between clean and adversarial images\n",
    "    across all layers of a ResNet model.\n",
    "    \"\"\"\n",
    "    diffs = {}\n",
    "\n",
    "    # Trace clean image\n",
    "    with model.trace(clean_img):\n",
    "        clean_main = _save_main_layers(model)\n",
    "        clean_blocks = {\n",
    "            **_save_block_outputs(model.layer1, \"layer1\"),\n",
    "            **_save_block_outputs(model.layer2, \"layer2\"),\n",
    "            **_save_block_outputs(model.layer3, \"layer3\"),\n",
    "            **_save_block_outputs(model.layer4, \"layer4\"),\n",
    "        }\n",
    "\n",
    "    # Trace adversarial image\n",
    "    with model.trace(adv_img):\n",
    "        adv_main = _save_main_layers(model)\n",
    "        adv_blocks = {\n",
    "            **_save_block_outputs(model.layer1, \"layer1\"),\n",
    "            **_save_block_outputs(model.layer2, \"layer2\"),\n",
    "            **_save_block_outputs(model.layer3, \"layer3\"),\n",
    "            **_save_block_outputs(model.layer4, \"layer4\"),\n",
    "        }\n",
    "\n",
    "    # Compute diffs for main layers\n",
    "    for name in clean_main:\n",
    "        diffs[name] = _compute_diffs(clean_main[name], adv_main[name])\n",
    "\n",
    "    # Compute diffs for residual blocks\n",
    "    for name in clean_blocks:\n",
    "        diffs[name] = _compute_diffs(clean_blocks[name], adv_blocks[name])\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    df = pd.DataFrame.from_dict(diffs, orient=\"index\").reset_index()\n",
    "    df.columns = [\"Layer\", \"Mean Absolute Difference\", \"Max Absolute Difference\"]\n",
    "\n",
    "    # Sorting logic based on hierarchical naming\n",
    "    def get_sort_index(layer_name):\n",
    "        parts = layer_name.split(\".\")\n",
    "        if layer_name.startswith(\"conv1\"):\n",
    "            return 0\n",
    "        elif layer_name.startswith(\"bn1\"):\n",
    "            return 1\n",
    "        elif layer_name.startswith(\"relu\"):\n",
    "            return 2\n",
    "        elif layer_name.startswith(\"maxpool\"):\n",
    "            return 3\n",
    "        elif \"layer\" in parts[0]:\n",
    "            layer_num = int(parts[0][-1])\n",
    "            block_num = int(parts[1][-1]) if len(parts) > 1 else -1\n",
    "            return 4 + layer_num * 10 + (block_num + 1 if block_num >= 0 else 0)\n",
    "        elif layer_name.startswith(\"avgpool\"):\n",
    "            return 100\n",
    "        elif layer_name.startswith(\"linear\"):\n",
    "            return 101\n",
    "        return 999  # fallback\n",
    "\n",
    "    df[\"SortIdx\"] = df[\"Layer\"].apply(get_sort_index)\n",
    "    df = df.sort_values(\"SortIdx\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(x=\"Layer\", y=\"Mean Absolute Difference\", data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\n",
    "        \"Mean Absolute Activation Differences Between Clean and Adversarial Images\"\n",
    "    )\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.barplot(x=\"Layer\", y=\"Max Absolute Difference\", data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\n",
    "        \"Maximum Absolute Activation Differences Between Clean and Adversarial Images\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"activation_differences.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'maxpool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualize_activation_differences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnsight_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh_eps_img\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mvisualize_activation_differences\u001b[0;34m(model, clean_img, adv_img)\u001b[0m\n\u001b[1;32m     10\u001b[0m mean_diffs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m max_diffs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(clean_img) \u001b[38;5;28;01mas\u001b[39;00m clean_tracer:\n\u001b[1;32m     14\u001b[0m     clean_conv1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     15\u001b[0m     clean_bn1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbn1\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[1;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:72\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     69\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(graph\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], graph, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mvisualize_activation_differences\u001b[0;34m(model, clean_img, adv_img)\u001b[0m\n\u001b[1;32m     14\u001b[0m clean_conv1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     15\u001b[0m clean_bn1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbn1\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m---> 16\u001b[0m clean_maxpool \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool\u001b[49m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     17\u001b[0m clean_layer1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     18\u001b[0m clean_layer2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayer2\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nnsight/intervention/base.py:441\u001b[0m, in \u001b[0;36mNNsight.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__methods__:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace(\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;241m*\u001b[39margs, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__methods__[key], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    439\u001b[0m     )\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_envoy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nnsight/intervention/envoy.py:670\u001b[0m, in \u001b[0;36mEnvoy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    657\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\n\u001b[1;32m    658\u001b[0m     Envoy[InterventionProxyType, InterventionNodeType], InterventionProxyType, Any\n\u001b[1;32m    659\u001b[0m ]:\n\u001b[1;32m    660\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper method for underlying module's attributes.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    If the attribute is a tensor (e.g. weights or bias) and accessed during tracing, then an InterventionProxy is created.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m        Union[InterventionProxyType, Any]: Attribute.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 670\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    673\u001b[0m         attr_proxy \u001b[38;5;241m=\u001b[39m protocols\u001b[38;5;241m.\u001b[39mParameterProtocol\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    674\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracer\u001b[38;5;241m.\u001b[39mgraph, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, key\n\u001b[1;32m    675\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'maxpool'"
     ]
    }
   ],
   "source": [
    "visualize_activation_differences(nnsight_model, clean_img, high_eps_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
